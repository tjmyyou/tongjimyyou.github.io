<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <link rel="stylesheet" href="tju.css" type="text/css" />
    <title>Selected Publications</title>
</head>
<body>
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    <table summary="Table for page layout." id="tlayout">
        <tr valign="top">
            <td id="layout-menu">
                <div class="image-container">
                    <img src="./projects/Tongji_University_Emblem.svg.png" width="96px" height="96px" alt="TJU">
                </div>
                <div class="menu-item"><a href="index.html">Homepage</a></div>
                <div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
                <div class="menu-item"><a href="group.html">Members</a></div>
                <div class="menu-item"><a href="service.html">Services</a></div>
                <div class="menu-item"><a href="award.html">Awards</a></div>
            </td>
            <td id="layout-content">
                <div id="toptitle">
                    <h1>Selected Pulications </h1>
                </div>
                <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <b><span style="font-family:Wingdings">*</span></b> corresponding author. All publications on [<a href="https://scholar.google.com/citations?user=cZ-SxbkAAAAJ&hl=zh-CN"
                        target=&ldquo;blank&rdquo;>Google Scholar</a>])
        <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
        <!-- 01 -->
		<tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10752365">
					<papertitle><b>Closely Cooperative Multi-Agent Reinforcement Learning Based on Intention Sharing and Credit Assignment</b></papertitle>
				</a>
				<br> Hao Fu, <b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>IEEE Robotics and Automation Letters</em>, 2024
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10752365">paper</a>/<a href="videos/01_fh.mp4">video</a>
				<br>
				<b>Abstract </b>Collaborative tasks are important in multi-agent systems. Multi-agent reinforcement learning is a commonly used technique for solving multi-agent cooperative policy learning. The closely collaborative task is a special but common case within cooperative tasks, where the change in the environmental state requires multiple agents to simultaneously perform specific actions. For example, in a box-pushing task where the boxes are heavy and require multiple agents to push simultaneously. The closely cooperative task faces some unique challenges. Firstly, the completion of a closely collaborative task requires agents to synchronize their actions, necessitating a consistent intention among them. Secondly, when some agents' erroneous actions lead to task failure, it becomes a challenge to avoid incorrectly penalizing agents who performed the correct actions. These challenges make most of the existing MARL methods perform poorly on this task. In this paper, we propose a closely collaborative multi-agent reinforcement learning(CC-MARL) algorithm based on intention sharing and credit assignment. We use a two-phase training to learn intention encoding and intention sharing respectively, and decompose joint action values based on counterfactual baseline ideas. We deployed scenarios in both simulated and real environments with various sizes, numbers of boxes, and numbers of agents and compare CC-MARL with various classical MARL algorithms on box-pushing tasks of different map scales in simulation, demonstrating the state-of-the-art of our method.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/01_fh.JPG" width=70%>
                </div>
            </td>
        </tr>
		<!-- 02 -->
        <tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://dl.acm.org/doi/full/10.1145/3702226">
					<papertitle><b>Physical-aware 3D Shape Understanding for Finishing Incomplete Assembly</b></papertitle>
				</a>
				<br> Weihao Wang, <b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>ACM Transactions on Graphics</em>, 2024
				<br>
				<a href="https://dl.acm.org/doi/full/10.1145/3702226">paper</a>/<a href="videos/02_wwh.mp4">video</a>
				<br>
				<br>
				<b>Abstract </b>Understanding the part composition and structure of 3D shapes is crucial for a wide range of 3D applications, including 3D part assembly and 3D assembly completion. Compared to 3D part assembly, 3D assembly completion is more complicated, which involves repairing broken or incomplete furniture that miss several parts with a toolkit. Given an incomplete assembly, 3D assembly completion seeks to identify its missing parts from multiple candidates, determine their poses, and produce complete assembly that is well-connected, structurally stable, and aesthetically pleasing. This task necessitates not only specialized knowledge of part composition but, more importantly, an awareness of physical constraints, i.e., connectivity, stability, and symmetry. Neglecting these constraints often results in assemblies that, although visually plausible, are impractical. To address this challenge, we propose PhysFiT, a physical-aware 3D shape understanding framework. This framework is built upon attention-based part relation modeling and incorporates connection modeling, simulation-free stability optimization and symmetric transformation consistency. We evaluate its efficacy on 3D part assembly and 3D assembly completion, a novel assembly task presented in this work. Extensive experiments demonstrate the effectiveness of PhysFiT in constructing geometrically sound and physically compliant assemblies.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/02_wwh.jpg" width=70%>
                </div>
            </td>
        </tr>
		<!-- 03 -->
        <tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681678">
					<papertitle><b>Scene Diffusion: Text-driven Scene Image Synthesis Conditioning on a Single 3D Model</b></papertitle>
				</a>
				<br> Xuan Han,Yihao Zhao,<b>Mingyu You*</b>
				<br>
				<em>ACM MM oral presentation (3.87%)</em>, 2024
				<br>
				<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681678">paper</a>/<a href="videos/03_hx.mp4">video</a>
				<br>
				<br>
				<b>Abstract </b>Scene image is one of the important windows for showcasing product design. To obtain it, the standard 3D-based pipeline requires designer to not only create the 3D model of product, but also manually construct the entire scene in software, which hindering its adaptability in situations requiring rapid evaluation. This study aims to realize a novel conditional synthesis method to create the scene image based on a single-model rendering of the desired object and the scene description. In this task, the major challenges are ensuring the strict appearance fidelity of drawn object and the overall visual harmony of synthesized image. The former's achievement relies on maintaining an appropriate condition-output constraint, while the latter necessitates a well-balanced generation process for all regions of image. In this work, we propose Scene Diffusion framework to meet these challenges. Its first progress is introducing the Shading Adaptive Condition Alignment (SACA), which functions as an intensive training objective to promote the appearance consistency between condition and output image without hindering the network's learning to the global shading coherence. Afterwards, a novel low-to-high Frequency Progression Training Schedule (FPTS) is utilized to maintain the visual harmony of entire image by moderating the growth of high-frequency signals in the object area. Extensive qualitative and quantitative results are presented to support the advantages of the proposed method. In addition, we also demonstrate the broader uses of Scene Diffusion, such as its incorporation with ControlNet.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/03_hx.JPG" width=70%>
                </div>
            </td>
        </tr>
		<!-- 04 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://arxiv.org/pdf/2410.08192">
					<papertitle><b>HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation</b></papertitle>
				</a>
				<br>Shanyan Guan, Yanhao Ge, Yin Tan, Jian Yang, Wei Li,<b>Mingyu You*</b>
				<br>
				<em>ECCV</em>, 2024
				<br>
				<a href="https://arxiv.org/pdf/2410.08192">paper</a>
				<br>
				<br>
				<b>Abstract</b> Recent advancements in text-to-image diffusion models have shown remarkable creative capabilities with textual prompts, but generating personalized instances based on specific subjects, known as subjectdriven generation, remains challenging. To tackle this issue, we present a new hybrid framework called HybridBooth, which merges the benefits of optimization-based and direct-regression methods. HybridBooth operates in two stages: the Word Embedding Probe, which generates a robust initial word embedding using a fine-tuned encoder, and the Word Embedding Refinement, which further adapts the encoder to specific subject images by optimizing key parameters. This approach allows for effective and fast inversion of visual concepts into textual embedding, even from a single image, while maintaining the model’s generalization capabilities.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/04_gyh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 05 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10543266">
					<papertitle><b>Contrast, Imitate, Adapt: Learning Robotic Skills from Raw Human Videos</b></papertitle>
				</a>
				<br>Zhifeng Qian,<b>Mingyu You*</b>,Hongjun Zhou, Xuanhui Xu, Hao Fu, Jinzhe Xue, Bin He
				<br>
				<em>IEEE Transactions on Automation Science and Engineering</em>, 2024
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10543266">paper</a>/<a href="videos/05_qzf.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Learning robotic skills from raw human videos remains a non-trivial challenge. Previous works tackled this problem by leveraging behavior cloning or learning reward functions from videos. Despite their remarkable performances, they may introduce several issues, such as the necessity for robot actions, requirements for consistent viewpoints and similar layouts between human and robot videos, as well as low sample efficiency. To this end, our key insight is to learn task priors by contrasting videos and to learn action priors through imitating trajectories from videos, and to utilize the task priors to guide trajectories to adapt to novel scenarios. We propose a three-stage skill learning framework denoted as Contrast-Imitate-Adapt (CIA). An interaction-aware alignment transformer is proposed to learn task priors by temporally aligning video pairs. Then a trajectory generation model is used to learn action priors. To adapt to novel scenarios different from human videos, the Inversion-Interaction method is designed to initialize coarse trajectories and refine them by limited interaction. In addition, CIA introduces an optimization method based on semantic directions of trajectories for interaction security and sample efficiency. The alignment distances computed by IAAformer are used as the rewards. We evaluate CIA in six real-world everyday tasks, and empirically demonstrate that CIA significantly outperforms previous state-of-the-art works in terms of task success rate and generalization to diverse novel scenarios layouts and object instances. Note to Practitioners —This work aims to study robot skill learning from raw human videos. Compared with teleoperation or kinesthetic teaching in the laboratory, such learning method can flexibly utilize large-scale human videos available on the Internet, thereby improving the robot’s ability to generalize to various complex scenarios. Previous works on learning from videos usually have some issues, including requirements for robot actions, consistent viewpoints, similar layouts and low sample efficiency. To alleviate these issues, we propose a three-stage skill learning framework CIA. Temporal alignment is utilized to learn task priors through our proposed transformer-based model and self-supervised loss functions. A trajectory generation model is trained to learn the action priors. To further adapt to diverse scenarios, we propose a two-stage policy improvement method by initialization and interaction. An optimization method is introduced to ensure safe interaction and sample efficiency, where the optimization objective is guided by the learned task priors. The experimental results show that our CIA outperforms other state-of-the-art methods in task success rate and generalization to novel scenarios.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/05_qzf.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 06 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/document/10568524">
					<papertitle><b>Cross-factory Polarizer Sheet Surface Defect Inspection System Based on Multi-Teacher Knowledge Amalgamation</b></papertitle>
				</a>
				<br><b>Mingyu You</b>,Baiyu Ren; Hongjun Zhou*
				<br>
				<em>IEEE Transactions on Instrumentation & Measurement</em>, 2024
				<br>
				<a href="https://ieeexplore.ieee.org/document/10568524">paper</a>  /<a href="https://github.com/Herrera21-a/MTKA">code</a> / <a href="videos/06_rby.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> To ensure the quality control of the polarizer sheet, surface defect inspection plays a pivotal role as the final step in the production process. Surface defect inspection of polarizer using auto optical inspection (AOI) poses several challenges, including the difficulty of the capturing clear and flat images of the polarizer, the presence of residue and dirt on the production line, which increases classification complexity, and variations in data across different production lines and factories, limiting model generalizability. To address these issues, this paper presents a polarizer sheet AOI equipment. With this equipment, we can capture images that are ideal for inspection purposes.  In addition, we introduce a novel approach that incorporates task-aligned and domain-adapted multi-teacher knowledge amalgamation for the first time based on the captured images.  Specifically, we introduce two selection strategies: one leveraging the Selection strategy based on judgement of student model (Stu-MKA) and the other leveraging the Selection strategy based on attention mechanism (Atn-MKA). These strategies guide the amalgamation of knowledge from diverse models, yielding a student model that harnesses the strengths of each teacher model, trained on disparate data domains. This approach effectively addresses the challenge of limited annotated samples in cross-factory training and the pronounced disparities in cross-domain data, culminating in commendable overall performance. The AOI equipment has been successfully applied to multiple production lines in multiple factories. It not only improves production capacity but also reduces human detection and operation errors, and the defect inspection results have received unanimous praise from end users. Our implementation is available at https://github.com/Herrera21-a/MTKA
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/06_rby.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 07 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.cambridge.org/core/journals/robotica/article/robot-imitation-from-multimodal-observation-with-unsupervised-crossmodal-representation/6B8AA9AC5470D433625BB7F795F091D6">
					<papertitle><b>Robot Imitation from Multimodal Observation with Unsupervised Cross-Modal Representation</b></papertitle>
				</a>
				<br>Xuanhui Xu,<b>Mingyu You*</b>,Hongjun Zhou, Bin He
				<br>
				<em>Robotica</em>, 2024
				<br>
				<a href="https://www.cambridge.org/core/journals/robotica/article/robot-imitation-from-multimodal-observation-with-unsupervised-crossmodal-representation/6B8AA9AC5470D433625BB7F795F091D6">paper</a> / <a href="videos/07_xxh.mp4">video</a> 
				<br>
				<br>
				<b>Abstract</b> Imitation from Observation (IfO) prompts the robot to imitate tasks from unlabeled videos via reinforcement learning (RL). The performance of the IfO algorithm depends on its ability to extract task-relevant representations since images are informative. Existing IfO algorithms extract image representations by using a simple encoding network or pre-trained network. Due to the lack of action labels, it is challenging to design a supervised task-relevant proxy task to train the simple encoding network. Representations extracted by a pre-trained network such as Resnet are often task-irrelevant. In this article, we propose a new approach for robot IfO via multimodal observations. Different modalities describe the same information from different sides, which can be used to design an unsupervised proxy task. Our approach contains two modules: the unsupervised cross-modal representation (UCMR) module and a self-behavioral cloning (self-BC)-based RL module. The UCMR module learns to extract task-relevant representations via a multimodal unsupervised proxy task. The Self-BC for further offline policy optimization collects successful experiences during the RL training. We evaluate our approach on the real robot pouring water task, quantitative pouring task, and pouring sand task. The robot achieves state-of-the-art performance.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/07_xxh.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 08 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10296870">
					<papertitle><b>Improving the Conditional Fine-grained Image Generation with Part Perception</b></papertitle>
				</a>
				<br>Xuan Han,<b>Mingyu You*</b>
				<br>
				<em>IEEE Transactions on Multimedia</em>, 2024
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10296870">paper</a> / <a href="videos/08_hx.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Synthesizing the images in line with the given condition is a cardinal issue of image generation. The fine-grained conditional image generation, due to its emphasis on the fidelity of details, is of profound worth to the studies in this field. To learn the conditional distribution of data, the discriminating to class semantic of generated samples is necessitated. Though, most existing methods realize it solely based on the condensed global feature, which potentially impedes the model's focus on the detailed local features and in turn causes the inaccuracy or unstable local appearances in generated images. In this context, we propose PartGAN, which features a novel part perception mechanism to strengthen the model's concentration on the nuts-and-bolts of fine-grained objects. In proposed method, the image given to the discriminator will be deconstructed and encoded into a set of embeddings that represent the semantics of parts. This scheme not only assists the model to capture the discriminative local features more accurately, but also prevents the omission of other general local features. Under the effect of the newly designed condition loss term, every part of generated image is equally encouraged to be closer to the corresponding real part, which helps to ensure that the general parts have a stable appearance that conforms to class semantic. The experiments on the popular benchmarks show that the proposed method significantly improves the effect of the generation for fine-grained images.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/08_hx.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 09 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://dl.acm.org/doi/abs/10.1145/3607541.3616815">
					<papertitle><b>Semi-supervised Learning with Easy Labeled Data via Impartial Labeled Set Extension</b></papertitle>
				</a>
				<br>Xuan Han,<b>Mingyu You*</b>
				<br>
				<em>ACM MM workshop</em>, 2023
				<br>
				<a href="https://dl.acm.org/doi/abs/10.1145/3607541.3616815">paper</a> / <a href="videos/09_hx.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Traditional Semi-supervised Learning (SSL) methods usually assume that the labeled data is independent and identically distributed (i.i.d.) from the underlying distribution. However, several relevant researches have revealed that i.i.d. assumption may not always hold. Inffuenced by the human preference or automatic labeling, in some cases, the labels (or trusted labels) would be concentrated in the easy samples which have distinctive characteristics. Such a biased labeled set will lead to grave misestimating for the decision boundaries in learning process. In this paper, we proposed a novel evolutionary SSL framework, Solar Eclipse (SE), to address the problem. This framework is based on the concept of progressively enlarging the labeled set with the closest unlabeled samples. Speciffcally, a novel relative distance measurement Regional Label Propagation (R-LP) is designed. In R-LP, the sample space is divided into several regions according to the class similarities, and the distance is calculated independently in each region with label propagation. Such segregation strategy efffciently reduces the complicity of distance measurement in the feature space. Moreover, R-LP also facilitates the ensemble of different feature views. In our practice, an unbiased self-supervised feature view is introduced to assist the measurement. Experiments show that such dual-view scheme can help us ffnd more reliable extending samples. The evaluation on the popular SSL benchmarks shows that the proposed SE framework achieves the most advanced performance with the easy labeled data. Except that, it also shows advantages when only a few i.i.d. labeled samples is provided, given that they may also have sampling bias.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/09_hx.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 10 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10355013">
					<papertitle><b>A Modular Framework for Robot Embodied Instruction Following by Large Language Model</b></papertitle>
				</a>
				<br>Long Li, Hongjun Zhou,<b>Mingyu You*</b>
				<br>
				<em>IEEE International Conference on Robotics & Biomimetics (ROBIO)</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10355013">paper</a> /<a href="https://leaderboard.allenai.org/alfred/submissions/public">submissions and results</a> 
				<br>
				<br>
				<b>Abstract</b> In the ALFRED challenge for robot simulation, the robot still faces a challenge to schedule a task in the embodied instruction following (EIF) tasks. These tasks require the robot to accurately perceive visual features and understand language instructions. However, the previous approaches typically employed end-to-end structures that utilize a shallow understanding of language instructions, while EIF tasks demand a deeper understanding of the semantic relationships in these instructions. To overcome these limitations, we propose a method which named REIF. The method incorporates modules for visual perception, language understanding, semantic search, closed container prediction, navigation, and operation to form a modular framework based on visual language multi-modal learning. The semantic search module supports more efficient object search, while the closed container prediction module enables deeper language understanding. Through learning multiple task instructions, the robot can efficiently and accurately complete EIF tasks in unseen scenes under certain step lengths. Our framework performs significantly well on unseen scene tasks within the ALFRED benchmark, achieving state-of-the-art accuracy and efficiency rates of 50.83% and 23.06% respectively. These results demonstrate that our method is capable of efficiently and accurately inferring the presence of closed container in unseen scenes, and can successfully execute a series of actions to interact with target object within closed container. Our method has achieved the first place in the ALFRED data-set competition. You can find our submissions and results at the following link: https://leaderboard.allenai.org/alfred/submissions/public.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/10.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 11 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10354761">
					<papertitle><b>A Real-Time Assistance System with Virtual-Real Fusion for Similar Components Assembly</b></papertitle>
				</a>
				<br>Jinzhe Xue,Hongjun Zhou,<b>Mingyu You*</b>
				<br>
				<em>IEEE International Conference on Robotics & Biomimetics (ROBIO)</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10354761">paper</a> / <a href="videos/11_xjz.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Assembly tasks are common in industrial production and daily life, however, paper instructions usually lack expressiveness and interactivity. In addition, components often have similarities in appearance and shape, which are not considered by traditional pose tracking methods and thus perform poorly. This paper proposes a real-time assembly assistance system, especially for similar parts assembly scenarios to improve working efficiency and user experience. The system obtains 6-DoF pose of each part for 3D synchronous display, detects assembly state, corrects wrong steps, and provides guidance for the next step. To cope with the problem of multiple similar parts obscuring each other during the process, our method VOSCO utilizes overall and positional consistency constraints to effectively segment target objects for the subsequent pose estimation FICP. Additionally, in the accelerated point cloud alignment operation, we compute the depth gradient by an optimized discrete differentiation operator and use it as the basis for point cloud sampling, which dramatically improves computational speed while maintaining alignment accuracy. Both VOSCO and FICP achieved good results in the custom dataset BP-SEG and BP-POSE for similar parts assembly scenarios, and the operation of the system satisfies real-time and good user experience.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/11_xjz.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 12 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/pii/S0921889023001914">
				   <papertitle><b>Task Parse Tree: Learning Task Policy from Videos with Task-irrelevant Components</b></papertitle>
				</a>
				<br>Weihao Wang,<b>Mingyu You*</b>
				<br>
				<em>Robotics and Autonomous Systems</em>, 2023
				<br>
				<a href="https://www.sciencedirect.com/science/article/pii/S0921889023001914">paper</a> / <a href="videos/demo_planning_independently.mp4">video:planning_independently</a>/ <a href="videos/demo_planning_interactively.mp4">video:planning_interactively</a>
				<br>
				<br>
				<b>Abstract</b> A good task policy should explicitly interpret the preconditions of actions and the composition structure of task. We aim to automatically learn such a task policy from videos which remains challenging at present. This issue can be further aggravated when task-irrelevant components are involved in videos, such as unoperated objects and small actions. Task-irrelevant objects may introduce disruptive visual relations, and task-irrelevant actions would lead to misleading and even failed task planning. Solving both issues simultaneously is beyond the scope of existing methods. To this end, we propose Task Parse Tree (TPT) as a novel task policy representation, distinguishing task-relevant actions with definite preconditions and clear execution order. The automatic generation of TPT relies on two core designs, where spatio-temporal graph (STG) seizes the vital changes in visual relations of objects and their attributes both spatially and temporally, and conjugate action graph (CAG) models the execution logic of actions in a graph. We collect a dataset of a real-world task, Make Tea, and experiment results on the dataset show that TPT realizes both accurate and interpretable task planning in two different scenarios.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/12_wwh.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		<!-- 13 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889023001057">
					<papertitle><b>Robot learning from human demonstrations with inconsistent contexts</b></papertitle>
				</a>
				<br>Zhifeng Qian,<b>Mingyu You*</b>, Hongjun Zhou, Xuanhui Xu, Bin He
				<br>
				<em>Robotics and Autonomous Systems</em>, 2023
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889023001057">paper</a> / <a href="videos/13_qzf.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Visual imitation learning is a promising approach that promotes robots to learn skills from visual demonstrations. However, current visual imitation learning approaches introduce unreasonable assumptions that the contexts of the visual demonstrations and the robot observations are consistent, which affects the flexibility and scalability of the approaches. It is a key challenge for robots to learn from visual demonstrations with inconsistent contexts. Inconsistent contexts may cause a serious difference in the pixel distribution of the operator and the environment, which makes vision-based control policies hardly effective. In this paper, we propose a novel imitation learning framework to enable robots to reproduce behavior by watching human demonstrations with inconsistent contexts, such as different viewpoints, operators, backgrounds, object appearances and positions. Specifically, our framework consists of three networks: flow-based viewpoint transformation network (FVTrans), robot2human alignment network (RANet) and inverse dynamics network (IDNet). First, FVTrans transforms various third-person demonstrations into the fixed robot execution view. With a meta learning strategy, FVTrans can quickly adapt to novel contexts with few samples. Then, RANet aligns the human and the robot at the feature level. Therefore, the demonstration feature can be used as a subgoal of the current moment. Finally, IDNet predicts the joint angles of the robot. We collect a multi-context dataset on the real robot (UR5) for three tasks, including grasping cups, sweeping garbage and placing objects. We empirically demonstrate that our framework can perform three tasks with a high success rate and be effectively generalized to different contexts.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/13_qzf.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 14 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10155203">
					<papertitle><b>Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning</b></papertitle>
				</a>
				<br>Zhifeng Qian,<b>Mingyu You*</b>Hongjun Zhou, Xuanhui Xu, Bin He
				<br>
				<em>IEEE Robotics and Automation Letters</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10155203">paper</a> / <a href="videos/14_qzf.mp4">video</a>
				<br>
				<br>
				<b>Abstract</b> Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) A state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the letter, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks. In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner. A simple Reachability Discrimination Module (REM) is also designed to determine the temporal distance of subgoals. Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training. We evaluate our REPlan in three vision-based simulation tasks and one real-world task. The experiments demonstrate that our REPlan significantly outperforms the prior state-of-the-art methods in solving temporally extended tasks.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/14_qzf.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 15 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10149371">
					<papertitle><b>GAN-Based Editable Movement Primitive from High-Variance Demonstrations</b></papertitle>
				</a>
				<br>Xuanhui Xu,<b>Mingyu You*</b>Hongjun Zhou, Bin He
				<br>
				<em>IEEE Robotics and Automation Letters</em>, 2023
				<br>https://github.com/Xu-Xuanhui/EditMP
				<a href="https://ieeexplore.ieee.org/abstract/document/10149371">paper</a> / <a href="videos/15_xxh.mp4">video</a>/ <a href="https://github.com/Xu-Xuanhui/EditMP">code</a>
				<br>
				<br>
				<b>Abstract</b> Movement Primitive (MP) is a promising Learning from Demonstration (LfD) framework, which is commonly used to learn movements from human demonstrations and adapt the learned movements to new task scenes. A major goal of MP research is to improve the adaptability of MP to various target positions and obstacles. MPs enable their adaptability by capturing the variability of demonstrations. However, current MPs can only learn from low-variance demonstrations. The low-variance demonstrations include varied target positions but leave various obstacles alone. These MPs can not adapt the learned movements to the task scenes with different obstacles, which limits their adaptability since obstacles are everywhere in daily life. In this letter, we propose a novel transformer and GAN-based Editable Movement Primitive (EditMP), which can learn movements from high-variance demonstrations. These demonstrations include the movements in the task scenes with various target positions and obstacles. After movement learning, EditMP can controllably and interpretably edit the learned movements for new task scenes. Notably, EditMP enables all robot joints rather than the robot end-effector to avoid hitting complex obstacles. The proposed method is evaluated on three tasks and deployed to a real-world robot. We compare EditMP with probabilistic-based MPs and empirically demonstrate the state-of-the-art adaptability of EditMP.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/15_xxh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 16 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://kns.cnki.net/kcms2/article/abstract?v=RyE_S26iMhVBOgLWeT_VgWSycGlLlYisx7hi6d11D2GoRFesICNb2zcx7bWKLFyQEhoAESFBoh1QNQXkAVySnkKXJMixShtuurk8jIezJ79LvLfuSkzL6SJqtcN9YHrAO6N7Qymf6iBB7GSEQw2j6d7IbnivMKPvg6LIdTRaW8oi8SahzJ8MNAaet0mehABF&uniplatform=NZKPT&language=CHS">
					<papertitle><b>基于稳定性优化的三维装配补全方法</b></papertitle>
				</a>
				<br>姚启皓,王伟昊,<b>尤鸣宇*</b>
				<br>
				<em>南京大学学报</em>, 2023
				<br>
				<a href="https://kns.cnki.net/kcms2/article/abstract?v=RyE_S26iMhVBOgLWeT_VgWSycGlLlYisx7hi6d11D2GoRFesICNb2zcx7bWKLFyQEhoAESFBoh1QNQXkAVySnkKXJMixShtuurk8jIezJ79LvLfuSkzL6SJqtcN9YHrAO6N7Qymf6iBB7GSEQw2j6d7IbnivMKPvg6LIdTRaW8oi8SahzJ8MNAaet0mehABF&uniplatform=NZKPT&language=CHS">paper</a>
				<br>
				<br>
				<b>摘要</b> 三维装配补全是一项重要的交互式装配任务，对于一个半成品装配体，机器人需要明确其缺失部件，从候选部件中挑选正确部件，计算准确的拼装位姿，最后将半成品补全.稳定性是椅子、桌子等实际装配体设计的首要原则，也是三维装配补全的重要目标，现有的装配补全工作多根据部件的几何关系来优化装配补全过程，没有考虑补全后装配体的稳定性，导致补全结果的正确率不高，难以满足机器人实际装配的需求.针对上述问题，提出一种基于稳定性优化的三维装配补全方法（Finishing the Incomplete 3D Assembly with Transformer,StableFiT），定义了一种装配体稳定性验证方法 .基于NVIDIA Isaac Sim仿真平台训练了一个装配体稳定性判别器，并基于稳定性判别器提供的稳定性反馈，优化了三维装配体的补全.在PartNet数据集上开展实验验证，结果表明StableFiT能够有效提升补全的装配体的正确性和稳定性.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/16_yqh.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 17 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10107717">
					<papertitle><b>MBFQuant: A Multiplier-Bitwidth-Fixed, Mixed Precision Quantization Method for Mobile CNN-Based Applications</b></papertitle>
				</a>
				<br>Peng Peng,<b>Mingyu You*</b>Kai Jiang, Youzao Lian, Weisheng Xu
				<br>
				<em>IEEE Transactions on Image Processing</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10107717">paper</a>
				<br>
				<br>
				<b>Abstract</b> Deploying Convolutional Neural Network (CNN)- based applications to mobile platforms can be challenging due to the conflict between the restricted computing capacity of mobile devices and the heavy computational overhead of running a CNN. Network quantization is a promising way of alleviating this problem. However, network quantization can result in accuracy degradation and this is especially the case with the compact CNN architectures that are designed for mobile applications. This paper presents a novel and efficient mixed-precision quantization pipeline, called MBFQuant. It redefines the design space for mixed-precision quantization by keeping the bitwidth of the multiplier fixed, unlike other existing methods, because we have found that the quantized model can maintain almost the same running efficiency, so long as the sum of the quantization bitwidth of the weight and the input activation of a layer is a constant. To maximize the accuracy of a quantized CNN model, we have developed a Simulated Annealing (SA)-based optimizer that can automatically explore the design space, and rapidly find the optimal bitwidth assignment. Comprehensive evaluations applying ten CNN architectures to four datasets have served to demonstrate that MBFQuant can achieve improvements in accuracy of up to 19.34% for image classification and 1.12% for object detection, with respect to a corresponding uniform bitwidth quantized model.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/17_pp.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 18 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/document/10017136">
					<papertitle><b>Design of an Efficient CNN-based Cough Detection System on Lightweight FPGA</b></papertitle>
				</a>
				<br>Peng Peng, Kai Jiang, <b>Mingyu You*</b>, Hongjun Zhou, Weisheng Xu
				<br>
				<em>IEEE Transactions on Biomedical Circuits and Systems</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/document/10017136">paper</a>
				<br>
				<br>
				<b>Abstract</b> Precisely and automatically detecting the cough sound is of vital clinical importance. Nevertheless, due to privacy protection considerations, transmitting the raw audio data to the cloud is not permitted, and therefore there is a great demand for an efficient, accurate, and low-cost solution at the edge device. To address this challenge, we propose a semi-custom software-hardware co-design methodology to help build the cough detection system. Specifically, we first design a scalable and compact convolutional neural network (CNN) structure that generates many network instances. Second, we develop a dedicated hardware accelerator to perform the inference computation efficiently, and then we find the optimal network instance by applying network design space exploration. Finally, we compile the optimal network and let it run on the hardware accelerator. The experimental results demonstrate that our model achieves 88.8% classification accuracy, 91.2% sensitivity, 86.5% specificity, and 86.5% precision, while the computation complexity is only 1.09M multiply-accumulation (MAC). Additionally, when implemented on a lightweight field programmable gate array (FPGA), the complete cough detection system only occupies 7.9 K lookup tables (LUTs), 12.9 K flip-flops (FFs), and 41 digital signal processing (DSP) slices, providing 8.3 GOP/s actual inference throughput and total power dissipation of 0.93 W. This framework meets the needs of partial application and can be easily extended or integrated into other healthcare applications.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/18_pp.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 19 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25365">
					<papertitle><b>3D Assembly Completion</b></papertitle>
				</a>
				<br>Weihao Wang, Rufeng Zhang<b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence</em>, 2023
				<br>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25365">paper</a>/ <a href="videos/19_wwh_1.mp4">vedio1</a>/ <a href="videos/19_wwh_2.mp4">vedio2</a>
				<br>
				<br>
				<b>Abstract</b> Automatic assembly is a promising research topic in 3D computer vision and robotics. Existing works focus on generating assembly (e.g., IKEA furniture) from scratch with a set of parts, namely 3D part assembly. In practice, there are higher demands for the robot to take over and finish an incomplete assembly (e.g., a half-assembled IKEA furniture) with an off-the-shelf toolkit, especially in human-robot and multi-agent collaborations. Compared to 3D part assembly, it is more complicated in nature and remains unexplored yet. The robot must understand the incomplete structure, infer what parts are missing, single out the correct parts from the toolkit and finally, assemble them with appropriate poses to finish the incomplete assembly. Geometrically similar parts in the toolkit can interfere, and this problem will be exacerbated with more missing parts. To tackle this issue, we propose a novel task called 3D assembly completion. Given an incomplete assembly, it aims to find its missing parts from a toolkit and predict the 6-DoF poses to make the assembly complete. To this end, we propose FiT, a framework for Finishing the incomplete 3D assembly with Transformer. We employ the encoder to model the incomplete assembly into memories. Candidate parts interact with memories in a memory-query paradigm for final candidate classification and pose prediction. Bipartite part matching and symmetric transformation consistency are embedded to refine the completion. For reasonable evaluation and further reference, we design two standard toolkits of different difficulty, containing different compositions of candidate parts. We conduct extensive comparisons with several baseline methods and ablation studies, demonstrating the effectiveness of the proposed method.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/19_wwh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 20 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322005039">
					<papertitle><b>Dynamic Dense CRF Inference for Video Segmentation and Semantic SLAM</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>Chaoxian Luo, Hongjun Zhou
				<br>
				<em>Pattern Recognition</em>, 2023
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322005039">paper</a>
				<br>
				<br>
				<b>Abstract</b> The dense conditional random field (dense CRF) is an effective post-processing tool for image/video segmentation and semantic SLAM. In this paper, we extend the traditional dense CRF inference algorithm to incremental sensor data modelling. The algorithm efficiently infers the maximum a posteriori probability (MAP) solution for a dynamically changing dense CRF model that is applied to incremental multi-class video segmentation and semantic SLAM. The computational cost is roughly proportional to the total change in the Gaussian pairwise edges of the dense CRF. In our system, with an increase in the number of frames of the sensor data, MAP calculations take approximately the same time to compute the overall three-dimensional dense CRF modelled for the entire video. Compared with the traditional dense CRF for video segmentation, this method is more suitable for incremental (in-line) video segmentation and robot semantic SLAM. The results of experiments show that if part of a pairwise edge is altered, our dynamic algorithm is significantly faster than the widely known standard dense CRF algorithm. In addition, the accuracy of its inference does not change. Several multi-class video segmentation tests confirmed the efficiency of inference of the algorithm. In another application, we used the dynamic dense CRF to incrementally integrate robot SLAM and video segmentation. The results show that an accurate SLAM can improve the accuracy of video segmentation, and the computational cost of the dense CRF MAP can be constrained over a constant range. The application of our algorithm is not limited to video segmentation: It is generic, and can be used to yield similar improvements in many optimization solutions for MAP in dynamically changing models.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/20.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 21 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/9944073">
					<papertitle><b>Robot Imitation Learning from Image-only Observation without Real world Interaction</b></papertitle>
				</a>
				<br>Xuanhui Xu,<b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>IEEE/ASME Transactions on Mechatronics</em>, 2023
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/9944073">paper</a>/ <a href="videos/21_xxh.mp4">vidio</a>
				<br>
				<br>
				<b>Abstract</b> Learning from observation (LfO) prompts the robot to imitate actions from experts' states via deep reinforcement learning (RL), achieving satisfactory results in simulation environments through hundreds of thousands of robot–environment interactions. While in the real world, constrained by the expensive and potentially dangerous interaction between the real robot and the environment, LfO is still challenging to be popularized. Therefore, reducing the number of interactions in LfO during training in the real world remains a hot research topic. Although significant progress has been made, the interaction is still inevitable. Hence, this article proposes the LION net ( Learning from Image-only Observation net ), which learns action from image-only demonstrations, e.g., a video of a human demonstrating a task, and reduces the number of the robot–environment interaction to zero in the real world. It is expected to be a realistic solution for real-world robot LfO. The LION net comprises two modules: 1) a domain transfer module that bridges the gap between the simulator and the real world and an RL-based control module that utilizes images as input to learning a task. The LION net affords the robot to imitate an action from the image-only human demonstration in the simulator and perform the learned action in the real world without additional training. The proposed method is evaluated on three real-life tasks: 1) pouring water, 2) washing cup, and 3) stacking cube, deployed to a real-world robot, demonstrating state-of-the-art performance.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/21_xxh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 22 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/9813581">
					<papertitle><b>3D Part Assembly Generation with Instance Encoded Transformer</b></papertitle>
				</a>
				<br>Rufeng Zhang, Tao Kong, Weihao Wang, Xuan Han, <b>Mingyu You*</b>
				<br>
				<em>IEEE Robotics and Automation Letters</em>, 2022
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/9813581">paper</a>
				<br>
				<br>
				<b>Abstract</b> It is desirable to enable robots capable of automatic assembly. Structural understanding of object parts plays a crucial role in this task yet remains relatively unexplored. In this letter, we focus on the setting of furniture assembly from a complete set of part geometries, which is essentially a 6-DoF part pose estimation problem. We propose a multi-layer transformer-based framework that involves geometric and relational reasoning between parts to update the part poses iteratively. We carefully design a unique instance encoding to solve the ambiguity between geometrically-similar parts so that all parts can be distinguished. In addition to assembling from scratch, we extend our framework to a new task called in-process part assembly. Analogous to furniture maintenance, it requires robots to continue with unfinished products and assemble the remaining parts into appropriate positions. Our method achieves far more than 10% improvements over the current state-of-the-art in multiple metrics on the public PartNet dataset. Extensive experiments and quantitative comparisons demonstrate the effectiveness of the proposed framework.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/22_wwh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 23 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321006816">
					<papertitle><b>Mask Encoding: A General Instance Mask Representation for Object Segmentation</b></papertitle>
				</a>
				<br>Rufeng Zhang, Tao Kong, Weihao Wang, Xuan Han, <b>Mingyu You*</b>
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>Pattern Recognition</em>, 2022
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321006816">paper</a>/<a href="https://git.io/AdelaiDet">code</a>
				<br>
				<br>
				<b>Abstract</b> Instance segmentation is one of the most challenging tasks in computer vision, which requires separating each instance in pixels. To date, a low-resolution binary mask is the dominant paradigm for representation of instance mask. For example, the size of the predicted mask in Mask R-CNN is usually. Generally, a low-resolution mask can not capture the object details well, while a high-resolution mask dramatically increases the training complexity. In this work, we propose a flexible and effective approach to encode the high-resolution structured mask to the compact representation which shares the advantages of high-quality and low-complexity. The proposed mask representation can be easily integrated into two-stage pipelines such as Mask R-CNN, improving mask AP by 0.9% on the COCO dataset, 1.4% on the LVIS dataset, and 2.1% on the Cityscapes dataset. Moreover, a novel single shot instance segmentation framework can be constructed by extending the existing one-stage detector with a mask branch for this instance representation. Our model shows its superiority over the explicit contour-based pipelines in accuracy with similar computational complexity. We also evaluate our method for video instance segmentation, achieving promising results on YouTube-VIS dataset. Code is available at: https://git.io/AdelaiDet
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/23_wwh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 24 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/9677980">
					<papertitle><b>Weakly Supervised Disentangled Representation for Goal-conditioned Reinforcement Learning</b></papertitle>
				</a>
				<br>Zhifeng Qian, <b>Mingyu You*</b>,Hongjun Zhou, Bin He
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>International Conference on Robotics and Automation (ICRA)</em>, 2022
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/9677980">paper</a>/<a href="vedios/24_qzf.mp4">vedio</a>
				<br>
				<br>
				<b>Abstract</b> Goal-conditioned reinforcement learning is a crucial yet challenging algorithm which enables agents to achieve multiple user-specified goals when learning a set of skills in a dynamic environment. However, it typically requires millions of the environmental interactions explored by agents, which is sample-inefficient. In the letter, we propose a skill learning framework DR-GRL that aims to improve the sample efficiency and policy generalization by combining the Disentangled Representation learning and Goal-conditioned visual Reinforcement Learning. In a weakly supervised manner, we propose a Spatial Transform AutoEncoder (STAE) to learn an interpretable and controllable representation in which different parts correspond to different object attributes (shape, color, position). Due to the high controllability of the representations, STAE can simply recombine and recode the representations to generate unseen goals for agents to practice themselves. The manifold structure of the learned representation maintains consistency with the physical position, which is beneficial for reward calculation. We empirically demonstrate that DR-GRL significantly outperforms the previous methods in sample efficiency and policy generalization. In addition, DR-GRL is also easy to expand to the real robot.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/24_qzf.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 25 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/pii/S1746809421009010">
					<papertitle><b>Automatic cough detection from realistic audio recordings using C-BiLSTM with boundary regression</b></papertitle>
				</a>
				<br><b>Mingyu You</b>WeihaoWang, You Li, Jiaming Liu, Xianghuai Xu, Zhongmin Qiu
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>Biomedical Signal Processing and Control</em>, 2022
				<br>
				<a href="https://www.sciencedirect.com/science/article/pii/S1746809421009010">paper</a>
				<br>
				<br>
				<b>Abstract</b> Automatic cough detection in the patients’ realistic audio recordings is of great significance to diagnose and monitor respiratory diseases, such as COVID-19. Many detection methods have been developed so far, but they are still unable to meet the practical requirements. In this paper, we present a deep convolutional bidirectional long short-term memory (C-BiLSTM) model with boundary regression for cough detection, where cough and non-cough parts need to be classified and located. We added convolutional layers before the LSTM to enhance the cough features and preserve the temporal information of the audio data. Considering the importance of the cough event integrity for subsequent analysis, the novel model includes an embedded boundary regression on the last feature map for both higher detection accuracy and more accurate boundaries. We delicately designed, collected and labelled a realistic audio dataset containing recordings of patients with respiratory diseases, named the Corp Dataset. 168 h of recordings with 9969 coughs from 42 different patients are included. The dataset is published online on the MARI Lab website (https://mari.tongji.edu.cn/info/1012/1030.htm). The results show that the system achieves a sensitivity of 84.13%, a specificity of 99.82% and an intersection-over-union (IoU) of 0.89, which is significantly superior to other related models. With the proposed method, all the criteria on cough detection significantly increased. The open source Corp Dataset provides useful material and a benchmark for researchers investigating cough detection. We propose the state-of-the-art system with boundary regression, laying the foundation for identifying cough sounds in real-world audio data.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/25_wwh.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 26 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/9244142">
					<papertitle><b>Part-Guided Attention Learning for Vehicle Instance Retrieval</b></papertitle>
				</a>
				<br>Xinyu Zhang, Rufeng Zhang, Jiewei Cao, Dong Gong,<b>Mingyu You*</b>,Chunhua Shen
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/9244142">paper</a>/<a href="https://github.com/zhangxinyu-xyz/PGAN-VehicleRe-ID?v=1">code</a>
				<br>
				<br>
				<b>Abstract</b> Vehicle instance retrieval (IR) often requires one to recognize the fine-grained visual differences between vehicles. Besides the holistic appearance of vehicles which is easily affected by the viewpoint variation and distortion, vehicle parts also provide crucial cues to differentiate near-identical vehicles. Motivated by these observations, we introduce a Part-Guided Attention Network (PGAN) to pinpoint the prominent part regions and effectively combine the global and local information for discriminative feature learning. PGAN first detects the locations of different part components and salient regions regardless of the vehicle identity, which serves as the bottom-up attention to narrow down the possible searching regions. To estimate the importance of detected parts, we propose a Part Attention Module (PAM) to adaptively locate the most discriminative regions with high-attention weights and suppress the distraction of irrelevant parts with relatively low weights. The PAM is guided by the identification loss and therefore provides top-down attention that enables attention to be calculated at the level of car parts and other salient regions. Finally, we aggregate the global appearance and local features together to improve the feature performance further. The PGAN combines part-guided bottom-up and top-down attention, global and local visual features in an end-to-end framework. Extensive experiments demonstrate that the proposed method achieves new state-of-the-art vehicle IR performance on four large-scale benchmark datasets.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/26.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 27 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16454">
					<papertitle><b>Diverse Knowledge Distillation for End-to-End Person Search</b></papertitle>
				</a>
				<br>Xinyu Zhang, Xinlong Wang, Jia-Wang Bian, Chunhua Shen,<b>Mingyu You*</b>
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>AAAI</em>, 2021
				<br>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16454">paper</a>/<a href="https://git.io/DKD-PersonSearch">code</a>
				<br>
				<br>
				<b>Abstract</b> Vehicle instance retrieval (IR) often requires one to recognize the fine-grained visual differences between vehicles. Besides the holistic appearance of vehicles which is easily affected by the viewpoint variation and distortion, vehicle parts also provide crucial cues to differentiate near-identical vehicles. Motivated by these observations, we introduce a Part-Guided Attention Network (PGAN) to pinpoint the prominent part regions and effectively combine the global and local information for discriminative feature learning. PGAN first detects the locations of different part components and salient regions regardless of the vehicle identity, which serves as the bottom-up attention to narrow down the possible searching regions. To estimate the importance of detected parts, we propose a Part Attention Module (PAM) to adaptively locate the most discriminative regions with high-attention weights and suppress the distraction of irrelevant parts with relatively low weights. The PAM is guided by the identification loss and therefore provides top-down attention that enables attention to be calculated at the level of car parts and other salient regions. Finally, we aggregate the global appearance and local features together to improve the feature performance further. The PGAN combines part-guided bottom-up and top-down attention, global and local visual features in an end-to-end framework. Extensive experiments demonstrate that the proposed method achieves new state-of-the-art vehicle IR performance on four large-scale benchmark datasets.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/27.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 28 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">
					<papertitle><b>Fully integer-based quantization for mobile convolutional neural network inference</b></papertitle>
				</a>
				<br>Peng Peng,<b>Mingyu You*</b>,Weisheng Xu, Jiaxin Li
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>Neurocomputing</em>, 2021
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">paper</a>/<a href="https://git.io/DKD-PersonSearch">code</a>
				<br>
				<br>
				<b>Abstract</b> Deploying deep convolutional neural networks on mobile devices is challenging because of the conflict between their heavy computational overhead and the hardware’s restricted computing capacity. Network quantization is typically used to alleviate this problem. However, we found that a “datatype mismatch” issue in existing low bitwidth quantization approaches can generate severe instruction redundancy, dramatically reducing their running efficiency on mobile devices. We therefore propose a novel quantization approach which ensures that only integer-based arithmetic is needed during the inference stage of the quantized model. To this end, we improved the quantization function to compel the quantized value to follow a standard integer format. Then we presented to simultaneously quantize the batch normalization parameters by a logarithm-like method. By doing so, the quantized model can keep the advantage of low bitwidth representation, while preventing the occurrence of “datatype mismatch” issue and corresponding instruction redundancy. Comprehensive experiments show that our method can achieve comparable prediction accuracy to other state-of-the-art methods while reducing the run-time latency by a large margin. Our fully integer-based quantized Resnet-18 has 4-bit weights, 4-bit activations and only a 0.7% top-1 and 0.4% top-5 accuracy drop on the ImageNet dataset. The assembly language implementation of a series of building blockscan reach a maximum of 4.33 the speed of the original full-precision version on an ARMv8 CPU.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/28.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 29 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">
					<papertitle><b>Visual Landmark Learning Via Attention-Based Deep Neural Networks</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>Chaoxian Luo, Hongjun Zhou
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>IEEE Transactions on Instrumentation and measurement</em>, 2021
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">paper</a>/<a href="https://github.com/dotstudio01/landmark_learning.git.">code</a>
				<br>
				<br>
				<b>Abstract</b> Compared with such high-precision and dense representation maps as the point cloud or occupancy grid maps, the landmark map has the advantages of compactness and memory efficiency, where these advantages are particularly prominent in a large-scale environment for robot localization, navigation, or environment measurement. However, training a robot to identify and select useful landmarks for localization is challenging. Due to this limitation, most landmarks are identified using handcrafted features. In this article, we propose a multitask neural network called LandmarkNet to build a flexible, optimal, and interpretable landmark map. The multitask neural network with an attention mechanism is trained for the robot's pose regression and the semantic segmentation of images from a set of sequences of monocular images. The image segmentation yields auxiliary semantic information for landmark selection based on some common-sense notions such as that the sky and clouds cannot be used as landmarks for robot localization. The attention feature map thus learned is used to select landmarks for robot localization based on the optimal projection between the image sequence with the corresponding pose of the robot and the semantic segmentation of the images. The learned landmark maps are also applied to mobile robot localization using traditional Monte Carlo localization (MCL). To verify the validity of our methods, we performed experiments on simulation, an open dataset (KITTI monoVO), and a dataset that we had created on the campus of Tongji University. The results show that the learned landmark map, which was only 6% of the size of the original map, can be used to accurately perform visual localization tasks. We open the source code on website https://github.com/dotstudio01/landmark_learning.git.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/29.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 30 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">
					<papertitle><b>Systematic evaluation of deep face recognition methods</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>Chaoxian Luo, Hongjun Zhou
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>Neurocomputing</em>, 2020
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319354">paper</a>/<a href="https://github.com/dotstudio01/landmark_learning.git.">code</a>
				<br>
				<br>
				<b>Abstract</b> Compared with such high-precision and dense representation maps as the point cloud or occupancy grid maps, the landmark map has the advantages of compactness and memory efficiency, where these advantages are particularly prominent in a large-scale environment for robot localization, navigation, or environment measurement. However, training a robot to identify and select useful landmarks for localization is challenging. Due to this limitation, most landmarks are identified using handcrafted features. In this article, we propose a multitask neural network called LandmarkNet to build a flexible, optimal, and interpretable landmark map. The multitask neural network with an attention mechanism is trained for the robot's pose regression and the semantic segmentation of images from a set of sequences of monocular images. The image segmentation yields auxiliary semantic information for landmark selection based on some common-sense notions such as that the sky and clouds cannot be used as landmarks for robot localization. The attention feature map thus learned is used to select landmarks for robot localization based on the optimal projection between the image sequence with the corresponding pose of the robot and the semantic segmentation of the images. The learned landmark maps are also applied to mobile robot localization using traditional Monte Carlo localization (MCL). To verify the validity of our methods, we performed experiments on simulation, an open dataset (KITTI monoVO), and a dataset that we had created on the campus of Tongji University. The results show that the learned landmark map, which was only 6% of the size of the original map, can be used to accurately perform visual localization tasks. We open the source code on website https://github.com/dotstudio01/landmark_learning.git.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/30.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 31 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Mask_Encoding_for_Single_Shot_Instance_Segmentation_CVPR_2020_paper.html">
					<papertitle><b>Mask Encoding for Single Shot Instance Segmentation</b></papertitle>
				</a>
				<br>Rufeng Zhang, Chunhua Shen<b>Mingyu You*</b>
				<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
				<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
				<br>
				<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Mask_Encoding_for_Single_Shot_Instance_Segmentation_CVPR_2020_paper.html">paper</a>/<a href="git.io/AdelaiDet">code</a>
				<br>
				<br>
				<b>Abstract</b> To date, instance segmentation is dominated by two-stage methods, as pioneered by Mask R-CNN. In contrast, one-stage alternatives cannot compete with Mask R-CNN in mask AP, mainly due to the difficulty of compactly representing masks, making the design of one-stage methods very challenging. In this work, we propose a simple single-shot instance segmentation framework, termed mask encoding based instance segmentation (MEInst). Instead of predicting the two-dimensional mask directly, MEInst distills it into a compact and fixed-dimensional representation vector, which allows the instance segmentation task to be incorporated into one-stage bounding-box detectors and results in a simple yet efficient instance segmentation framework. The proposed one-stage MEInst achieves 36.4% in mask AP with single-model (ResNeXt-101-FPN backbone) and single-scale testing on the MS-COCO benchmark. We show that the much simpler and flexible one-stage instance segmentation method, can also achieve competitive performance. This framework can be easily adapted for other instance-level recognition tasks. Code is available at: git.io/AdelaiDet
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/31.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		
		<!-- 32 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://link.springer.com/article/10.1007/s11554-020-00980-1">
					<papertitle><b>Fast contour detection with supervised attention learning</b></papertitle>
				</a>
				<br>Rufeng Zhang,<b>Mingyu You*</b>
				<br>
				<em>Journal of Real-Time Image Processing</em>, 2020
				<br>
				<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681678">paper</a>
				<br>
				<br>
				<b>Abstract</b> Recent advances in deep convolutional neural networks have led to significant success in many computer vision tasks, including edge detection. However, the existing edge detectors neglected the structural relationships among pixels, especially those among contour pixels. Inspired by human perception, this work points out the importance of learning structural relationships and proposes a novel real-time attention edge detection (AED) framework. Firstly, an elaborately designed attention mask is employed to capture the structural relationships among pixels at edges. Secondly, in the decoding phase of our encoder–decoder model, a new module called dense upsampling group convolution is designed to tackle the problem of information loss due to stride downsampling. And then, the detailed structural information can be preserved even it is ever destroyed in the encoding phase. The proposed relationship learning module introduces negligible computation overhead, and as a result, the proposed AED meets the requirement of real-time execution with only 0.65M parameters. With the proposed model, an optimal dataset scale F-score of 79.5 is obtained on the BSDS500 dataset with an inference speed of 105 frames per second, which is significantly faster than existing methods with comparable accuracy. In addition, a state-of-the-art performance is achieved on the BSDS500 (81.6) and NYU Depth (77.0) datasets when using a heavier model.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/32.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 33 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html">
					<papertitle><b>Self-training with progressive augmentation for unsupervised cross-domain person re-identification</b></papertitle>
				</a>
				<br>Xinyu Zhang, Jiewei Cao, Chunhua Shen,<b>Mingyu You*</b>
				<br>
				<em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019
				<br>
				<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html">paper</a>/<a href="tinyurl.com/PASTReID">code</a>
				<br>
				<br>
				<b>Abstract</b> Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function based on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: tinyurl.com/PASTReID
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/33.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 34 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/8961570">
					<papertitle><b>Simultaneous multi-task determination and manipulation with vision-based self-reasoning network</b></papertitle>
				</a>
				<br>Zhifeng Qian, Xuanhui Xu,<b>Mingyu You*</b>,Hongjun Zhou
				<br>
				<em>IEEE International Conference on Robotics and Biomimetics, ROBIO 2019, EI会议</em>
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/8961570">paper</a>
				<br>
				<br>
				<b>Abstract</b> It remains a great challenge for robots to simultaneously determine the task objective and schedule the corresponding manipulation, which requires robots to autonomously perceive and understand the environments, think up a response strategy and plan a set of trajectories or actions. This paper proposes a muti-task self-reasoning neural network (MSRnet), which is an end-to-end imitation learning framework consisting of a self-reasoning module and a control module. The self-reasoning module supports the task objective inference, while the control module predicts the robot motor angles for manipulation. With multi-task learning, MSRnet enables robots to accomplish multiple tasks with one model without updating parameters. This paper takes three tasks as an example, involving pouring water into a cup of coffee powder, taking a spoon and stirring coffee with the spoon, which simulate the application scene of making coffee. We employ a low-cost robotic arm(less than $300) to evaluate our MSRnet on coffee maker(CM) dataset which is collected by ourselves. MSRnet with 640*480 resolution inputs can achieve 83.3% success rate on multi-task test, 76.7% success rate on complex environment test. The result verifies that MSRnet can accurately infer the task objective and generate corresponding task actions simultaneously.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/34.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 35 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/8310009">
					<papertitle><b>An Extended Filtered Channel Framework for Pedestrian Detection</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>,Yubin Zhang, Chunhua Shen, Xinyu Zhang
				<br>
				<em>IEEE Transactions on Intelligent Transportation Systems</em>, 2018
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/8310009">paper</a>
				<br>
				<br>
				<b>Abstract</b> Pedestrian detection is an important example of object detection and has attracted much attention. Many works have shown that good image features provide high detection accuracy, and a few works have investigated enhancing low-level features (e.g., gradient and color features) using a filtered layer (i.e., convolutional layer) to obtain enhanced features or filtered channel features. To investigate whether these features are saturated, this paper adopts the concept of filtered channel features and strengthens them by adding more convolutional layers. Acting as convolution kernels, multilayer filters are applied to low-level features to obtain the extended filtered channel features, providing a powerful feature extractor with multilayer transformation for pedestrian detection. The proposed extended filtered channel framework (ExtFCF) achieves competitive performance on widely used benchmark datasets (Caltech, INRIA, and KITTI datasets), using only histogram of oriented gradient (HOG) and CIE-LUV [a color space composing of luminance (L) and two chrominance (UV) components by International Commission on Illumination] color features (HOG+LUV) as low-level features. One representative ExtFCF implementation achieves the best result compared with the current best traditional pedestrian detection methods on the Caltech dataset.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/35.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 36 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885618300155">
					<papertitle><b>Reading Car License Plates Using Deep Neural Networks</b></papertitle>
				</a>
				<br>Hui Li, Peng Wang,<b>Mingyu You*</b>Chunhua Shen
				<br>
				<em>Image and Vision Computing</em>, 2018
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885618300155">paper</a>
				<br>
				<br>
				<b>Abstract</b> In this work, we tackle the problem of car license plate detection and recognition in natural scene images based on the powerful deep neural networks (DNNs). Firstly, a 37-class convolutional neural network (CNN) is trained to detect characters in an image, which leads to a high recall compared with a binary text/non-text classifier. False positives are then eliminated effectively by a plate/non-plate CNN classifier. As to the license plate recognition, we regard the character string reading as a sequence labeling problem. Recurrent neural networks (RNNs) with long short-term memory (LSTM) are trained to recognize the sequential features extracted from the whole license plate via CNNs. The main advantage of this approach is that it is segmentation free. By exploring contextual information and avoiding errors caused by segmentation, this method performs better than conventional methods and achieves state-of-the-art recognition accuracy.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/36.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 37 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885618300155">
					<papertitle><b>Sequential data feature selection for human motion recognition via Markov blanket</b></papertitle>
				</a>
				<br>Hongjun Zhou, <b>Mingyu You*</b>, Lei Liu, Chao Zhuang
				<br>
				<em>Pattern Recognition Letters</em>, 2017
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885618300155">paper</a>
				<br>
				<br>
				<b>Abstract</b> Human motion recognition is a hot topic in the field of human–machine interface research, where human motion is often represented in time sequential sensor data. This paper investigates human motion recognition based on feature-selected sequential Kinect skeleton data. We extract features from the Cartesian coordinates of human body joints for machine learning and recognition. As there are errors associated with the sensor, in addition to other uncertain factors, human motion sequential sensor data usually includes some irrelative and error features. To improve the recognition rate, an effective method is to reduce the amount of irrelative and error features from original sequential data. Feature selection methods for static situations, such as photo images, are widely used. However, very few investigations in the literature discuss this with regards to sequential data models, such as HMM (Hidden Markov Model), CRF (Conditional Random Field), DBN (Dynamic Bayesian Network), and so on. Here, we propose a novel method which combines a Markov blanket with the wrapper method for sequential data feature selection. The proposed algorithm is assessed using four sets of open human motion data and two types of learners (HMM and DBN), and the results show that it yields better recognition accuracy than traditional methods and non-feature selection models.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/37.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 38 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S1746809416301835">
					<papertitle><b>Cough Detection by Ensembling Multiple Frequency Subband Features</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>, Zeqin Liu, Chong Chen, Jiaming Liu, Xianghuai Xu, Zhongmin Qiu
				<br>
				<em>Biomedical Signal Processing and Control</em>, 2017
				<br>
				<a href="https://www.sciencedirect.com/science/article/abs/pii/S1746809416301835">paper</a>
				<br>
				<br>
				<b>Abstract</b> Cough is a common symptom in respiratory diseases. Objectively evaluating the quantity and intensity of cough by pattern recognition technologies can provide valuable clinical information for cough diagnosis and monitoring. Cough detection is the basis of cough diagnosis and analysis. It aims at detecting cough events and their exact boundaries from an audio stream. From signal characteristics, it is found that energy distribution scatters in the cough spectrum, which is obviously different from speech signals. However, almost all feature extraction methods for cough detection in previous works are derived from the speech recognition domain. In this article, subband features are obtained by using gammatone filterbank and an audio feature extraction method. Support Vector Machine (SVM), K-Nearest Neighbors (KNN) and Random Forest (RF) are trained with the corresponding subband features and ensemble method combines the outputs to make the final decision. Experiments are conducted on both synthetic data and real data. The real data is collected from 18 patients with respiratory diseases in clinical environments and annotated by human experts. Experiment results demonstrate that ensembling multiple frequency subbands helps to impove performance in cough detection. Compared with other methods, our method can improve the accuracy by 3.2%.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/38.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 39 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-spr.2016.0341">
					<papertitle><b>A Novel Feature Extraction Method for Cough Detection using Non-Negative Matrix Factorization</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>, Huihui Wang, Chong Chen, Jiaming Liu, Xianghuai Xu, Zhongmin Qiu
				<br>
				<em>IET Signal Processing</em>, 2017
				<br>
				<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-spr.2016.0341">paper</a>
				<br>
				<br>
				<b>Abstract</b> Cough is a common symptom in respiratory diseases. Objectively evaluating the quantity and intensity of cough by pattern recognition technologies can provide valuable clinical information for cough diagnosis and monitoring. Cough detection is the basis of cough diagnosis and analysis. It aims at detecting cough events and their exact boundaries from an audio stream. From signal characteristics, it is found that energy distribution scatters in the cough spectrum, which is obviously different from speech signals. However, almost all feature extraction methods for cough detection in previous works are derived from the speech recognition domain. In this article, subband features are obtained by using gammatone filterbank and an audio feature extraction method. Support Vector Machine (SVM), K-Nearest Neighbors (KNN) and Random Forest (RF) are trained with the corresponding subband features and ensemble method combines the outputs to make the final decision. Experiments are conducted on both synthetic data and real data. The real data is collected from 18 patients with respiratory diseases in clinical environments and annotated by human experts. Experiment results demonstrate that ensembling multiple frequency subbands helps to impove performance in cough detection. Compared with other methods, our method can improve the accuracy by 3.2%.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/39.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 40 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-spr.2016.0341">
					<papertitle><b>ISMAC: An Intelligent System for Customized Clinical Case Management and Analysis</b></papertitle>
				</a>
				<br><b>Mingyu You*</b>,  Chong Chen, Guozheng Li, Shixing Yan, Sheng Sun, Xueqiang Zeng, Qingce Zhao, Liaoyu Xu, Suying Huang
				<br>
				<em>The Scientific World Journal</em>, 2015
				<br>
				<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-spr.2016.0341">paper</a>
				<br>
				<br>
				<b>Abstract</b> Clinical cases are primary and vital evidence for Traditional Chinese Medicine (TCM) clinical research. A great deal of medical knowledge is hidden in the clinical cases of the highly experienced TCM practitioner. With a deep Chinese culture background and years of clinical experience, an experienced TCM specialist usually has his or her unique clinical pattern and diagnosis idea. Preserving huge clinical cases of experienced TCM practitioners as well as exploring the inherent knowledge is then an important but arduous task. The novel system ISMAC (Intelligent System for Management and Analysis of Clinical Cases in TCM) is designed and implemented for customized management and intelligent analysis of TCM clinical data. Customized templates with standard and expert-standard symptoms, diseases, syndromes, and Chinese Medince Formula (CMF) are constructed in ISMAC, according to the clinical diagnosis and treatment characteristic of each TCM specialist. With these templates, clinical cases are archived in order to maintain their original characteristics. Varying data analysis and mining methods, grouped as Basic Analysis, Association Rule, Feature Reduction, Cluster, Pattern Classification, and Pattern Prediction, are implemented in the system. With a flexible dataset retrieval mechanism, ISMAC is a powerful and convenient system for clinical case analysis and clinical knowledge discovery.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/40.jpg" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 41 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://link.springer.com/article/10.1186/1472-6947-15-S4-S2">
					<papertitle><b>Cough Event Classification by Pretrained Deep Neural Network</b></papertitle>
				</a>
				<br>Jia-Ming Liu,<b>Mingyu You*</b>, Zheng Wang, Guozheng Li, Xianghuai Xu, Zhongmin Qiu
				<br>
				<em>BMC Medical Informatics and Decision Making</em>, 2015
				<br>
				<a href="https://link.springer.com/article/10.1186/1472-6947-15-S4-S2">paper</a>
				<br>
				<br>
				<b>Abstract</b> Cough is an essential symptom in respiratory diseases. In the measurement of cough severity, an accurate and objective cough monitor is expected by respiratory disease society. This paper aims to introduce a better performed algorithm, pretrained deep neural network (DNN), to the cough classification problem, which is a key step in the cough monitor.The deep neural network models are built from two steps, pretrain and fine-tuning, followed by a Hidden Markov Model (HMM) decoder to capture tamporal information of the audio signals. By unsupervised pretraining a deep belief network, a good initialization for a deep neural network is learned. Then the fine-tuning step is a back propogation tuning the neural network so that it can predict the observation probability associated with each HMM states, where the HMM states are originally achieved by force-alignment with a Gaussian Mixture Model Hidden Markov Model (GMM-HMM) on the training samples. Three cough HMMs and one noncough HMM are employed to model coughs and noncoughs respectively. The final decision is made based on viterbi decoding algorihtm that generates the most likely HMM sequence for each sample. A sample is labeled as cough if a cough HMM is found in the sequence.In this paper, we tried pretrained deep neural network in cough classification problem. Our results showed that comparing with the conventional GMM-HMM framework, the HMM-DNN could get better overall performance on cough classification task.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/41.JPG" width=70%>
		        </div>
		    </td>
		</tr>
		
		<!-- 42 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/7359724">
					<papertitle><b>Audio Signals Encoding for Cough Classification Using Convolutional Neural Networks: A Comparative Study</b></papertitle>
				</a>
				<br>Hui-Hui Wang, Jiaming Liu,<b>Mingyu You*</b>, Guozheng Li
				<br>
				<em>IEEE International Conference Bioinformatics and Biomedicine (BIBM)</em>, 2015
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/7359724">paper</a>
				<br>
				<br>
				<b>Abstract</b> Cough detection has considerable clinical value, which can provide an objective basis for assessment and diagnosis of respiratory diseases. Motivated by the great achievements of convolutional neural networks (CNNs) in recent years, we adopted 5 different ways to encode audio signals as images and treated them as the input of CNNs, so that image processing technology could be applied to analyze audio signals. In order to explore the optimal audio signals encoding method, we performed comparative experiments on medical dataset containing 70000 audio segments from 26 patients. Experimental results show that RASTA-PLP spectrum is the best method to encode audio signals as images with respect to cough classification task, which gives an average accuracy of 0.9965 in 200 iterations on test batches and a F1-score of 0.9768 on samples re-sampled from the test set. Therefore, the image processing based method is shown to be a promising choice for the process of audio signals.
				<p></p>
		        <br>
		    </td>
		</tr>
		
		<!-- 43 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/7419077">
					<papertitle><b>Markov blanket based sequential data feature selection for human motion recognition</b></papertitle>
				</a>
				<br>Chao Zhuang, Hongjun Zhou, <b>Mingyu You*</b>, Lei Liu
				<br>
				<em>IEEE International Conference on Robotics & Biomimetics (ROBIO)</em>, 2015
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/7419077">paper</a>
				<br>
				<br>
				<b>Abstract</b> Human motion recognition is a hot topic in the field of human-machine interface research, where human motion is often represented in time sequential sensor data. This paper investigates human motion recognition based on feature-selected sequential Kinect skeleton data. We extract features from the Cartesian coordinates of human body joints for machine learning and recognition. As there are errors associated with the sensor, in addition to other uncertain factors, human motion sequential sensor data usually includes some irrelative and error features. To improve the recognition rate, an effective method is to reduce the amount of irrelative and error features from original sequential data. Feature selection methods for static situations, such as photo images, are widely used. However, very few investigations in the literature discuss this with regards to sequential data models, such as HMM (Hidden Markov Model), CRF (Conditional Random Field), DBN (Dynamic Bayesian Network), and so on. Here, we propose a novel method which combines a Markov blanket with the wrapper method for sequential data feature selection. The proposed algorithm is assessed using four sets of human motion data and two types of learners (HMM and DBN), and the results show that it yields better recognition accuracy than traditional methods and non-feature selection models.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/43.JPG" width=70%>
		        </div>
		    </td>
		</tr>

        </table>
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
			<td style="padding:10px;width:75%;vertical-align:top;text-align:justify;">
			(1)	尤鸣宇;韩煊; 一种基于伪标签噪声过滤的小样本半监督学习方法和装置；2022-1-7，中国，ZL 2022 1 0015285.0
			<br>(2)	钱智丰;尤鸣宇; 一种基于无监督图像编辑的多目标强化学习方法；2022-4-28，中国，ZL 2022 1 0469373.8 
			<br>(3)	尤鸣宇;韩煊; 一种基于无监督多模型融合的时序任务预测方法和装置；2022-1-21，中国，ZL 2022 1 0071568.7 
			<br>(4)	尤鸣宇;周虹旭;钱智丰;周洪钧; 基于第三视角可变主体演示视频的机械臂模仿学习方法, 2021-02-26, 中国, ZL 2021 1 0218017.4
			<br>(5)	尤鸣宇;温佳豪;周洪钧; 基于多模态信息的机器人准确抓取方法及计算机可读介质, 2021-02-26, 中国, ZL 2021 1 0218016.X
			<br>(6)	尤鸣宇;王伟昊;周洪钧; 一种基于选择性知识传递的双特长教师模型知识融合方法, 2021-02-26, 中国, ZL 2021 1 0218021.0 
			<br>(7)	尤鸣宇;钱智丰;周洪钧; 一种基于语言引导的机械臂动作模仿学习系统, 2021-02-26, 中国, ZL 2021 1 0217079.3 
			<br>(8)	尤鸣宇;苏志成;周洪钧; 一种基于动态模型强化学习算法的倒水服务机器人, 2021-02-26, 中国, ZL 2021 1 0217090.X
			<br>(9)	尤鸣宇;徐炫辉;周洪钧; 一种基于模仿学习的服务机器人定量倒水算法, 2021-02-26, 中国, ZL 2021 1 0217089.7 
			<br>(10) 李由;尤鸣宇; 一种经过压缩的咳嗽自动检测方法及嵌入式设备, 2020-12-30, 中国, ZL 2020 1 1617737.X
			<br>(11) 张佳伟;尤鸣宇; 一种使用残差注意力机制网络的同步定位与建图方法, 2019-11-28, 中国, ZL 2019 1 1190243.5
			<br>(12) 尤鸣宇;沈春华;张欣彧; 基于triplet深度二值网络的快速人脸检索方法, 2018-01-11, 中国, ZL 2018 1 0026049.2
			<br>(13) 尤鸣宇;沈春华;徐杨柳; 一种基于cnn和卷积LSTM网络的行人再识别方法, 2016-06-21, 中国, ZL 2016 1 0450898.1
			<br>(14) 刘泽琴;尤鸣宇; 一种基于加速度计的咳嗽判别系统, 2015-06-09, 中国, ZL 2015 2 0393651.1
			</td>
		</table>
</body>
</html>